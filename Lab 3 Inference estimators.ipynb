{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference of Parameters in Parametric Models\n",
    "\n",
    "In this lab, we will look at estimating parameters based on observation of data points and deepen our understanding of error estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the parameters of Gaussian Data\n",
    "\n",
    "We will generate $n$ data points, which are i.i.d. realizations of a random variable $X$ that follows a normal distribution, i.e. $X\\sim\\mathcal{N}(\\mu, \\sigma^2)$.\n",
    "\n",
    "\n",
    "$$p(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{ - \\frac{(x - \\mu)^2}{2\\sigma^2} }$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 1  # Mean of the distribution\n",
    "sigma = 10  # Standard deviation of the distribution\n",
    "\n",
    "\n",
    "num_samples = 50  # Number of data points to generate\n",
    "\n",
    "# Generate synthetic data points\n",
    "data = np.random.normal(loc=mu, scale=sigma, size=num_samples)\n",
    "\n",
    "#Visualization of the datapoints\n",
    "plt.plot(data,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, we examined the point estimators $\\hat\\mu$ and $\\hat\\sigma^2$, which stand for $\\mu$ and $\\sigma^2$, respectively. The expressions for these estimators are given by:\n",
    "$$\\hat\\mu = \\bar X$$\n",
    "and\n",
    "$$\\hat\\sigma^2 = S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar X)^2$$\n",
    "Both of these estimators are unbiased and consistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also obtained that the Maximum Likelihood Estimation (MLE) estimator for $\\sigma^2$ was\n",
    "$$\\hat \\sigma_\\mathrm{MLE}=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar X)^2$$\n",
    "which, on the contrary to $\\hat\\sigma$ is biased, but it is better with respect to the MSE criterion.\n",
    "\n",
    "**Exercise:** Let us now construct a function that uses these formulas to estimate $\\hat\\mu$, $\\hat\\sigma^2$ and $\\hat \\sigma_\\mathrm{MLE}$ using a vector of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gaussian_parameters(data):\n",
    "    n=len(data)\n",
    "    data = np.array(data)\n",
    "    \n",
    "    hmu = ...\n",
    "    hsigma2 = ...\n",
    "    hsigma2_mle = ...\n",
    "    \n",
    "    return hmu, hsigma2,hsigma2_mle \n",
    "\n",
    "hmu, hsigma2, hsigma2_mle = gaussian_parameters(data)\n",
    "\n",
    "print(\"Estimation of mu: {:.4f}\".format(hmu),\"True {:.4f}\".format(mu))\n",
    "print(\"Estimation of sigma^2: {:.4f}\".format(hsigma2),\"True {:.4f}\".format(sigma**2))\n",
    "print(\"MLE estimation of sigma^2: {:.4f}\".format(hsigma2_mle),\"True {:.4f}\".format(sigma**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Visualize the data in a histogram and compare it with the true and inferred distribution. Rerun the experiment several times to see how it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(x,mu,sigma2):\n",
    "    return 1./np.sqrt(2*np.pi*sigma2)*np.exp(-(x-mu)**2/(2*sigma2))\n",
    "\n",
    "# Visualizing the generated data using a histogram\n",
    "plt.hist(data, bins=10, align='mid', density=True, alpha=0.5, color='g')\n",
    "\n",
    "# Add a title\n",
    "plt.title('Histogram of Generated Gaussian Data')\n",
    "\n",
    "x=np.linspace(np.min(data),np.max(data))\n",
    "plt.plot(x,...,color='black',label='True')\n",
    "plt.plot(x,...,label=r'Inferred $\\hat\\sigma^2$')\n",
    "plt.plot(x,...,label=r'Inferred $\\hat\\sigma^2_\\mathrm{MLE}$')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If now you rerun the experiment (you generate a new dataset) and recompute the estimators, you will obtain different values for all three estimators. The estimators are random variables themselves. The important question is being able to quantify the degree of error to the estimates obtained.\n",
    "\n",
    "###  Investigating the sampling distribution\n",
    "\n",
    "**Exercise:** Admitting that $\\text{Var}(S^2)=2\\sigma^4/(n-1)$, compute the Bias and the MSE for both estimators $\\hat\\sigma^2$ and $\\hat\\sigma^2_{\\text{MLE}}$, as a function of $n$. Which of the two estimators for $\\sigma^2$ has smaller MSE?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can write your calculations, for that, transform the cell to Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3: Sampling distribution:** Which is the expected sampling distribution for the three estimators?\n",
    "\n",
    "For this goal, recall that we are dealing with multiple independent realizations of a random variable $X\\sim \\mathcal{N}(\\mu,\\sigma^2)$. \n",
    "\n",
    "In this context, you can use first the fact that the sum of two randomly distributed random variables $X$ and $Y$ is also Gaussianly distributed and\n",
    "$$X\\sim \\mathcal{N}\\left(\\mu_X,\\sigma_X^2\\right)$$\n",
    "$$Y\\sim \\mathcal{N}\\left(\\mu_Y,\\sigma_Y^2\\right)$$\n",
    "then\n",
    "$$Z=X+Y\\sim \\mathcal{N}\\left(\\mu_X+\\mu_Y,\\sigma_X^2+\\sigma_Y^2\\right)$$\n",
    "and use it to obtain the distribution of $\\bar X$\n",
    "\n",
    "For the variance estimates, you should use the Cochran's theorem. If $Z_1,\\ldots, Z_n$ are independent i.i.d., standard normal random variables, i.e. $Z_i\\sim \\mathcal{N}\\left(0,1\\right)$, then\n",
    "$$Z=\\sum_{i=1}^n \\left( Z_i-\\bar X\\right)^2 \\sim \\chi^2 (n-1)$$\n",
    "with $\\chi^2(n-1)$ being the $\\chi^2$ function with $n-1$ degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can write your calculations, for that, transform the cell to Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Empirically estimate of the sampling distribution** Repeat the experiment $N=10^4$ times and save the value of the estimators obtained in each trial in a vector. Repeat this using different dataset sizes $n=10,100,1000,10^4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10000\n",
    "\n",
    "list_num_samples=np.array([10,100,1000,10000])\n",
    "\n",
    "xmu,xs2,xs2_mle=np.zeros((3,len(list_num_samples),N))\n",
    "\n",
    "for i,n in enumerate(list_num_samples):\n",
    "    for t in range(N):\n",
    "        data = np.random.normal(loc=mu, scale=sigma, size=n)\n",
    "        ...\n",
    "\n",
    "        xmu[i,t]=...\n",
    "        xs2[i,t]=...\n",
    "        xs2_mle[i,t]= ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compare empirical estimation of the bias and MSE as a function of $n$ with the\n",
    "with the theoretical ones obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_mu,bias_sigma2,bias_sigma2_mle=np.zeros((3,len(list_num_samples)))\n",
    "mse_mu,mse_sigma2,mse_sigma2_mle=np.zeros((3,len(list_num_samples)))\n",
    "\n",
    "for i,n in enumerate(list_num_samples):\n",
    "    \n",
    "    bias_mu[i]=np.average(xmu[i])-mu\n",
    "    bias_sigma2[i]=...\n",
    "    bias_sigma2_mle[i]=...\n",
    "    \n",
    "    mse_mu[i]=np.average((xmu[i]-mu)**2)\n",
    "    mse_sigma2[i]=...\n",
    "    mse_sigma2_mle[i]=...\n",
    "    \n",
    "    \n",
    "plt.title('Bias')\n",
    "plt.plot(list_num_samples,bias_mu,color='C0',label=r'$\\hat\\mu$')\n",
    "plt.plot(list_num_samples,bias_sigma2,color='C1',label=r'$\\hat\\sigma2$')\n",
    "plt.plot(list_num_samples,bias_sigma2_mle,color='C2',label=r'$\\hat\\sigma2_\\mathrm{MLE}$')\n",
    "\n",
    "\n",
    "plt.plot(list_num_samples,...,':',color='C0')\n",
    "plt.plot(list_num_samples,...,':',color='C1')\n",
    "plt.plot(list_num_samples,...,':',color='C2')\n",
    "\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title('MSE')\n",
    "plt.plot(list_num_samples,mse_mu,label=r'$\\hat\\mu$',color='C0')\n",
    "plt.plot(list_num_samples,mse_sigma2,label=r'$\\hat\\sigma2$',color='C1')\n",
    "plt.plot(list_num_samples,mse_sigma2_mle,label=r'$\\hat\\sigma2_\\mathrm{MLE}$',color='C2')\n",
    "plt.xlabel(r'$n$')\n",
    "\n",
    "ns=np.array(list_num_samples)\n",
    "\n",
    "plt.plot(ns,sigma**2/ns,':',color='C0')\n",
    "\n",
    "plt.plot(ns,2*sigma**4/(ns-1),':',color='C1')\n",
    "\n",
    "plt.plot(ns,sigma**4*(2*ns-1)/(ns**2),':',color='C2')\n",
    "\n",
    "\n",
    "plt.xlabel(r'$n$')\n",
    "plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compare the density histograms of the estimators obtained for each value of $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i,n in enumerate(list_num_samples):\n",
    "    plt.hist(xmu[i],bins=50,alpha=0.5,color='C'+str(i),label='n='+str(n),density=True)\n",
    "    x=np.array(xmu[i])\n",
    "    \n",
    "plt.axvline(x=mu,color='black',label='True')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i,n in enumerate(list_num_samples):\n",
    "    plt.hist(xs2[i],bins=50,alpha=0.5,color='C'+str(i),label='n='+str(n),density=True)\n",
    "plt.legend()\n",
    "plt.axvline(x=sigma**2,color='black',label='True')\n",
    "plt.show()\n",
    "\n",
    "for i,n in enumerate(list_num_samples):\n",
    "    plt.hist(xs2_mle[i],bins=50,alpha=0.5,color='C'+str(i),label='n='+str(n),density=True)\n",
    "    \n",
    "plt.axvline(x=sigma**2,color='black',label='True')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compare the normalized histograms with the theoretical distribution expected. \n",
    "In order to compare the distribution of the variance estimators, you need to properly readjust the variables. Recall that the Cochran's theorem tells us that\n",
    "\n",
    "$Z=\\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2(n-1)\\Rightarrow f(z)=\\chi^2_{n-1}(z)$. But we are interested in the distribution of $S^2$, not $Z$. In order to change variables, mind that the measure must be conserved, i.e. $f(z)dz=g(S^2)dS^2$. \n",
    "\n",
    "The $\\chi^2$ for $n-1$ degrees of freedom is included in the scipy.stats package, $\\chi^2_{n-1}(z)=$chi2.pdf(z, n-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "for i,n in enumerate(list_num_samples):\n",
    "    plt.hist(xmu[i],bins=50,alpha=0.5,color='C'+str(i),label='n='+str(n),density=True)\n",
    "    x=np.linspace(np.min(xmu[i]),np.max(xmu[i]),1000)\n",
    "    \n",
    "    plt.plot(x,...,color='C'+str(i))\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "for i,n in enumerate(list_num_samples):\n",
    "    plt.hist(xs2[i],bins=50,alpha=0.5,color='C'+str(i),label='n='+str(n),density=True)\n",
    "    \n",
    "    \n",
    "    plt.plot(...,...,color='C'+str(i))\n",
    "\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "for i,n in enumerate(list_num_samples):\n",
    "    \n",
    "    plt.hist(xs2_mle[i],bins=50,alpha=0.5,color='C'+str(i),label='n='+str(n),density=True)\n",
    "    \n",
    "    \n",
    "    plt.plot(...,...,color='C'+str(i))\n",
    "    \n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error of an estimator is defined as \n",
    "\n",
    "$$\\text{SE}(\\hat\\theta)=\\sqrt{\\text{Var}(\\hat\\theta)}$$\n",
    "\n",
    "In order to compute it exactly, we need to know the sampling distribution of our variable, which is normally unknown but we know it in our case.\n",
    "\n",
    "**Exercise:** Obtain the theoretical standard error for $\\hat\\mu$ and $\\hat\\sigma$. Remain that $\\text{Var}(S^2)=\\frac{2\\sigma^4}{n-1}$. Propose the corresponding estimated standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can write your calculations, for that, transform the cell to Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compare $\\hat\\mu$ and $\\hat\\sigma^2$ estimated for different values of $n$ with the expected error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000\n",
    "\n",
    "data = np.random.normal(loc=mu, scale=sigma, size=n)\n",
    "hmu, hsigma2, hsigma2_mle = gaussian_parameters(data)\n",
    "\n",
    "print(\"Estimation of mu: {:.4f}\".format(hmu),\"+-{:.4f}\".format(...),\"True {:.4f}\".format(mu))\n",
    "print(\"Estimation of sigma^2: {:.4f}\".format(hsigma2),\"+-{:.4f}\".format(...),\"True {:.4f}\".format(sigma**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence intervals are an important tool for estimating the range in which a parameter value lies with a certain degree of confidence. Let us consider a 95% confidence interval to make it even clearer. The interpretation is that if we were to run the same experiment several times, we would expect the true parameter value to lie within this interval 95% of the time, or equivalently, with probability 0.95.\n",
    "\n",
    "We have established the following relationships for Gaussian variables:\n",
    "\n",
    "$$\\frac{\\bar{X}-\\mu}{\\sqrt{\\sigma^2/n}}\\sim \\mathcal{N}(0,1)$$\n",
    "\n",
    "$$\\frac{(n-1)S^2}{\\sigma^2}\\sim \\chi^2(n-1)$$\n",
    "\n",
    "By merging these expressions, we derive the distribution for the t-statistic:\n",
    "\n",
    "$$t=\\frac{\\bar{X}-\\mu}{\\hat{\\text{SE}}(\\bar{X})}=\\frac{\\bar{X}-\\mu}{\\frac{S}{\\sqrt{n}}}\\sim t(n-1),$$\n",
    "\n",
    "Where \\(t(n-1)\\) means the t-Student distribution with \\(n-1\\) degrees of freedom.\n",
    "\n",
    "This means that our true parameters are expressed as follows:\n",
    "\n",
    "$$\\mu=\\bar{X} +\\eta \\frac{S}{\\sqrt{n}}\\quad\\text{with}\\quad\\eta\\sim t(n-1)$$\n",
    "\n",
    "$$\\sigma^2=\\frac{(n-1)S^2}{\\eta}\\quad\\text{with}\\quad\\eta\\sim \\chi^2(n-1)$$\n",
    "\n",
    "Since the probability density functions (pdf) of these $\\eta$ are known, we can use these expressions to define an interval in which the estimators will lie with $1-\\alpha$ probability. To do this, we need to find the quantiles $ x_{\\alpha/2} $ and $x_{1-\\alpha/2} $ for each distribution.\n",
    "\n",
    "Since the t-Student distribution is symmetric,\n",
    "$$-x_{\\alpha/2} = x_{1-\\alpha/2} \\equiv t_{1-\\frac{\\alpha}{2}, n-1}$$\n",
    "and we will use\n",
    "$$\\chi_{\\alpha/2}^2(n-1) \\text{ and } \\chi_{1-\\alpha/2}^2(n-1)$$\n",
    "to represent the quantiles of the $ \\chi^2(n-1)$ distribution.\n",
    "\n",
    "Thus, the confidence intervals (CI) for each estimator are:\n",
    "\n",
    "$$CI_{1-\\alpha}(\\mu) = \\bar{X} \\pm t_{1-\\alpha/2, n-1} \\frac{S}{\\sqrt{n}}$$\n",
    "$$CI_{1-\\alpha}(\\sigma^2) = \\left( \\frac{(n-1)S^2}{\\chi_{1-\\alpha/2, n-1}^2}, \\frac{(n-1)S^2}{\\chi_{\\alpha/2, n-1}^2} \\right)$$\n",
    "\n",
    "In these expressions, $ CI_{1-\\alpha}(\\mu)$ and $ CI_{1-\\alpha}(\\sigma^2)$ denote the confidence intervals for the estimators of $\\mu$ and $\\sigma^2$, which ensure that the true parameter with a confidence level of $ 1 - \\alpha$ is included in these ranges.\n",
    "\n",
    "**Exercise:** Obtain the 95% confidence intervals for $\\mu$ and $\\sigma^2$ using a set of $n$ data points. The quantiles of the $t$ and $\\chi^2$ distributions we can obtain them using the scipy.stats library, and the function ppf($\\alpha$, degrees of freedom) for each that returns the $\\alpha$-quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import t,chi2\n",
    "# Estimating standard errors\n",
    "\n",
    "n=100\n",
    "\n",
    "data = np.random.normal(loc=mu, scale=sigma, size=n)\n",
    "hmu, hsigma2, hsigma2_mle = gaussian_parameters(data)\n",
    "\n",
    "\n",
    "# Calculating 95% confidence intervals\n",
    "confidence_level = 0.5\n",
    "alpha=1-confidence_level\n",
    "\n",
    "hse_mu = np.sqrt(hsigma2/n)\n",
    "df = n - 1  # degrees of freedom\n",
    "\n",
    "quantile_t= t.ppf(1-alpha/2, df)\n",
    "\n",
    "lower_quantile_chi=chi2.ppf(...,)\n",
    "higher_quantile_chi= chi2.ppf(...,)\n",
    "\n",
    "\n",
    "ci_mu = (...,...)\n",
    "\n",
    "ci_sigma2 = (...,...)\n",
    "\n",
    "\n",
    "print(\"{:.2f}% CI for mu: ({:.4f}, {:.4f})\".format(confidence_level ,ci_mu[0], ci_mu[1]))\n",
    "print(\"{:.2f}% CI for sigma^2: ({:.4f}, {:.4f})\".format(confidence_level ,ci_sigma2[0], ci_sigma2[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** We can now verify that the true value of the parameters in the CI thus obtained is indeed with probability$1-\\alpha$ by repeating the same experiment $N=10000$. Each time we calculate the CI and check whether it contains the true values or not. Then we calculate the frequency with which it is contained. Try different confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mu=0\n",
    "n_sigma2=0\n",
    "N=1000\n",
    "\n",
    "\n",
    "confidence_level = 0.75\n",
    "for i in range(N):\n",
    "    \n",
    "    n=100\n",
    "    data = np.random.normal(loc=mu, scale=sigma, size=n)\n",
    "    hmu, hsigma2, hsigma2_mle = gaussian_parameters(data)\n",
    "\n",
    "    ...\n",
    "    \n",
    "    if mu...:\n",
    "        n_mu+=1\n",
    "        \n",
    "    if sigma...:\n",
    "        n_sigma2+=1\n",
    "        \n",
    "print(r'mu is contained with a frequency {:.2f}'.format(n_mu/N))\n",
    "print(r'sigma2 is contained with a frequency {:.2f}'.format(n_sigma2/N))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bootstrap Method:\n",
    "\n",
    "Bootstrap is a resampling method that allows us to approximate the sampling distribution of any estimator/statistic. It works on the principle that the empirical distribution (observed data) is the best representation we have of the true distribution of the population.\n",
    "\n",
    "#### Detailed Steps:\n",
    "\n",
    "1. **New Sample**:\n",
    "\n",
    " For observed data, we draw $n$ samples **with replacement**. This means that a single data point may be selected more than once, and some data points may not be selected at all. This redrawn data set is called the bootstrap sample. From this bootstrap sample, we compute the estimator (call it $\\hat\\theta^* $).\n",
    "\n",
    "2. **Repeat**:\n",
    "\n",
    " We repeat the above step $B$ times, resulting in $B$ bootstrap samples. For each of these samples, we compute a different $ \\hat\\theta^{*(b)} $. This gives us $ B$ different estimators based on the bootstrap samples.\n",
    "\n",
    "3. **Bootstrap sampling distribution**:\n",
    "\n",
    " The collection of all $ \\hat\\theta^{*(b)} $ values gives us an empirical representation of the sampling distribution of $ \\hat\\theta $. It is empirical because it is derived from our actual data.\n",
    "\n",
    "4. **Approximation to the sampling law**:\n",
    "\n",
    " This step is the basis for making the bootstrap method work. The main idea is that the differences between the bootstrap estimates $ \\hat\\theta^{*(b)} $ and the original estimate $ \\hat\\theta $ approximate the differences we would expect between the estimator $\\hat\\theta$ and the true parameter $\\theta$. Mathematically, this is expressed as follows:\n",
    " $$\\text{Law} (\\hat\\theta-\\theta) \\approx \\text{Bootstrap sampling law}(\\hat\\theta^{*(b)}-\\hat\\theta) $$\n",
    " \n",
    " If you increase $ B $, this approximation becomes better.\n",
    "\n",
    "5. **Empirical Bootstrap Quantiles**:\n",
    "\n",
    " Here we try to find the percentiles of the bootstrap distribution that contain the middle $(1-\\alpha) $ fraction of the data. More specifically, the $(1-\\alpha/2)$ quantiles (for a $(1-\\alpha)$ confidence level).\n",
    "\n",
    " To illustrate: Suppose you have 1000 bootstrap estimates. At a 95% confidence level, you would sort these estimates and take the 25th and 975th estimates as your $\\alpha/2$ and $(1-\\alpha) $ estimates, respectively. $(1-\\alpha/2)$ quantile, respectively.\n",
    "\n",
    "6. **Define Confidence Interval**:\n",
    "\n",
    " And now for the derivation of this confidence interval:\n",
    "\n",
    " Consider the difference between the original estimate, \\( \\hat\\theta \\), and the bootstrap estimates, $ \\hat\\theta^*$. If we assume that the estimator is symmetrically distributed about the true parameter value (a common assumption), then we would expect the original estimate, $\\hat\\theta$, to be as far above the true parameter, $ \\theta $, as the bootstrap estimate, $\\hat\\theta^* $ is above $ \\hat\\theta$. This is the reason for the \"reflection\" in the confidence interval:\n",
    " $$IC (\\theta) = \\left[ 2\\hat\\theta - b^*_{1-\\alpha/2}, 2\\hat\\theta - b^*_{\\alpha/2} \\right]$$\n",
    "\n",
    " The reflection ensures that the variability and potential bias captured by the bootstrap samples are correctly reflected in the confidence interval.\n",
    "\n",
    "**Exercise** Obtain the confidence interval for $\\mu$ and $\\sigma^2$ using Boostrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def bootstrap_estimates(original_data, alpha, num_bootstrap_samples=1000):\n",
    "    \n",
    "    n = len(original_data)\n",
    "    hmu, hsigma2, _ = gaussian_parameters(original_data)\n",
    "    \n",
    "    hmu_bootstrap = np.zeros(num_bootstrap_samples)\n",
    "    hsigma2_bootstrap = np.zeros(num_bootstrap_samples)\n",
    "    \n",
    "    for i in range(num_bootstrap_samples): # Generate a bootstrap sample\n",
    "        bootstrap_sample = np.random.choice(original_data, size=n, replace=True)\n",
    "        hmu_bootstrap[i], hsigma2_bootstrap[i], _ = ...\n",
    "    \n",
    "    lower_quantile_mu = np.percentile(hmu_bootstrap, alpha/2*100)\n",
    "    higher_quantile_mu = np.percentile(hmu_bootstrap, (1-alpha/2)*100)\n",
    "    \n",
    "    ci_b_mu = (...,...)\n",
    "    \n",
    "    lower_quantile_sigma2 = np.percentile(hsigma2_bootstrap, alpha/2*100)\n",
    "    higher_quantile_sigma2 = np.percentile(hsigma2_bootstrap, (1-alpha/2)*100)\n",
    "    ci_b_sigma2 = (..., ...)\n",
    "    \n",
    "    return ci_b_mu, ci_b_sigma2\n",
    "\n",
    "\n",
    "\n",
    "confidence_level=0.95\n",
    "alpha=1-confidence_level\n",
    "ci_mu_bootstrap, ci_sigma2_bootstrap = bootstrap_estimates(data, alpha)\n",
    "\n",
    "print(\"Bootstrap {:.2f}% CI for mu: ({:.2f}, {:.2f})\".format(100*(1-alpha), ci_mu_bootstrap[0], ci_mu_bootstrap[1]))\n",
    "print(\"Bootstrap {:.2f}% CI for sigma^2: ({:.2f}, {:.2f})\".format(100*(1-alpha), ci_sigma2_bootstrap[0], ci_sigma2_bootstrap[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Check that we obtain very similar CIs that those obtained before using the properties of the Gaussian random variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
