{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary predictors are a key tool in classification problems. These predictors are central to a variety of applications, from disease diagnosis to financial forecasting. In this computer lab, we will attempt to assess the quality of a set of binary predictors on balanced and unbalanced datasets using the quality estimators and metrics we defined in the previous class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setup\n",
    "\n",
    "We will create an artificial dataset with two clusters in high dimensions. We will assign a label 0 and 1 to each of these clusters and the goal is to evaluate the performance of different predictors to classify which entry belongs to each cluster. In the lab, we will firstly evaluate the quality of the predictions and secondly fine tune the parameters to optimize the performance of each predictor. In the end, we will decide which predictor is the best of all.\n",
    "\n",
    "We plan to study the following predictors:\n",
    "\n",
    "#### 1. Random assignment\n",
    "\n",
    "- **Strategy:** Randomly assign label 1 based on a predefined probability \\( p \\).\n",
    "- **Parameter to Tune:** The probability \\( p \\) of choosing label 1.\n",
    "\n",
    "#### 2. PCA-based predictor\n",
    "\n",
    "- **Strategy:** Use principal component analysis (PCA) to assign label 1 based on the first principal component (PC1).\n",
    "- **Parameter to Tune:** The threshold \\( a \\) so that label 1 is selected when PC1 < \\( a \\).\n",
    "\n",
    "#### 3. Logistic Regression\n",
    "\n",
    "- **Strategy:** Implement logistic regression to estimate the probability that a given entry belongs to label 1.\n",
    "- **Tuning parameter:** The threshold for determining the label based on the estimated probability.\n",
    "\n",
    "---\n",
    "\n",
    "## Next steps:\n",
    "\n",
    "1. Define our data set\n",
    "2. Define the different predictors\n",
    "3. Define the various quality metrics discussed in the previous class.\n",
    "4. Evaluate each predictor and optimize the parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generation of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate 2 random clusters for a binary test classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Setting the probability for the true class\n",
    "Prob_true = 0.7\n",
    "\n",
    "# Generating the n_samples in 20 dimensions belonging to 2 categories  \n",
    "X, y = make_classification(n_features=20,n_samples=10000, n_classes=2, #n_clusters_per_class=1,class_sep=1.5,\n",
    "                           weights=[1-Prob_true,Prob_true],random_state=0)\n",
    "\n",
    "print(X.shape)\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "\n",
    "\n",
    "# Plotting the generated data\n",
    "plt.scatter(X[:, 0], X[:, 1], marker='o', c=y, edgecolors='k')\n",
    "plt.xlabel('first dir')\n",
    "plt.ylabel('second dir')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that there are two clases of data-points using dimensional reduction, for instance the PCA (we will study its meaning later on in the course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# Plotting the training data in the new PCA space\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], marker='o', c=y_train, edgecolors='k')\n",
    "plt.title('PCA Projection of the Training Data')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "index_1=(y_train==1)\n",
    "index_0=(y_train==0)\n",
    "\n",
    "\n",
    "plt.hist(X_train_pca[index_1, 0],bins=50,color='yellow',alpha=0.5,label='true')\n",
    "plt.hist(X_train_pca[index_0, 0],bins=50,color='purple',alpha=0.5,label='false')\n",
    "plt.hist(X_train_pca[:, 0],bins=50,color='black',alpha=0.2,label='all')\n",
    "plt.legend()\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define different predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define our 3 different predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No skills\n",
    "\n",
    "I want a predictor that just assign 1 with a random probability $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_skills(X,p=0.5):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive: PCA based predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_based(X,threshold):\n",
    "    pca = PCA(n_components=1)\n",
    "    X_pca = pca.fit_transform(X).reshape(len(X))\n",
    "    \n",
    "    return (X_pca<threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can learn to classify this data using logistic regression. I include a discussion about its training but we do not need this for this lab.\n",
    "\n",
    "Let us describe the data in terms of a probability function\n",
    "$$p_{A,B}(\\boldsymbol{x})=\\frac{1}{1+e^{-(A+\\boldsymbol{B}\\cdot\\boldsymbol{x})}},$$ using labeled data $(\\boldsymbol{x}_k,y_k)$. We want to train the model such that the probability $p_k=p_{A,B}(\\boldsymbol{x}_k)=1$ for $y_k=1$ and 0 for $y_k=0$. For this purpose, we can define a log loss\n",
    "$$L=-\\sum_{k} y_k\\log p_k -(1-y_k)\\log p_k $$\n",
    "(it is clear that $L$ is minimized $p_k=1$ when $y_k=1$ and $p_k=0$ when $y_k=0$).\n",
    "One can then minimize it by calculating the gradient\n",
    "$$\\frac{\\partial L}{\\partial A}=\\sum_k (p_k-y_k)=0$$\n",
    "$$\\frac{\\partial L}{\\partial \\boldsymbol{B}}=\\sum_k \\boldsymbol{x}_k (p_k-y_k)=0$$\n",
    "Which is solved numerically. As a result of this process, we obtain a way to assign to each data point a probability of being in the category $y=1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic(X_test,X_train,y_train):\n",
    "\n",
    "    clf = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_probs = clf.predict_proba(X_test)\n",
    "\n",
    "    return y_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion matrix\n",
    "\n",
    "Now we have a set of two pairs mutually incompatible events: \n",
    "\n",
    "- **True cluster** being part of cluster $[0,1]$,\n",
    "- **Predicted label**: getting $[-,+]$\n",
    "\n",
    "\n",
    "### Confusion matrices\n",
    "\n",
    "We need to compute the conditional probablities, e.g. $P(1|+)$, or $P(0|+)$.\n",
    "\n",
    "Define a function that given the number two vectors containing the categories of the samples in the test set, $y$ and $y_{\\rm pred}$, it returns the number of true positives (TP, $N(1\\cap +)$), true negatives (TN, $N(0\\cap -)$), false positives (FP, $N(0\\cap +)$)  and false negatives (FN, $N(1\\cap -)$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(y_pred,y_test):\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return TN,FP,FN,TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the confusion matrix \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Pred -</th>\n",
    "    <th>Pred +</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Real 0</th>\n",
    "    <td>TNR </td>\n",
    "    <td>FPR </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Real 1</th>\n",
    "    <td>FNR </td>\n",
    "    <td>TPR</td>\n",
    "  </tr>\n",
    "</table>\n",
    "It is more convenient to divide by the total number of entries to have an idea of the proportions.\n",
    "\n",
    "\n",
    "for 3 different predictors:\n",
    "    \n",
    "- Random guess with $p=0.99$\n",
    "- PCA threshold 0\n",
    "- Logistic $p_1>0.5\\rightarrow 1$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_r=...\n",
    "y_pred_pca=...\n",
    "\n",
    "probs1_log=... # probability of 1\n",
    "y_pred_l=(probs1_log>0.5).astype(int)\n",
    "\n",
    "titles=['no skills','pca','logistic']\n",
    "\n",
    "fig, ax = plt.subplots(1, 3,figsize=(15,5))\n",
    "for i,y_pred in enumerate([y_pred_r, y_pred_pca,y_pred_l]):\n",
    "    \n",
    "    TN,FP,FN,TP=compute_confusion_matrix(y_test,y_pred)\n",
    "    cm=[[TN,FP],[FN,TP]]\n",
    "\n",
    "    ax[i].set_title(titles[i])\n",
    "    sns.heatmap(cm/np.sum(cm), annot=True, cmap=\"Blues\", cbar=False,ax=ax[i])\n",
    "    ax[i].set_xlabel('Predicted')\n",
    "    ax[i].set_ylabel('Real')\n",
    "plt.suptitle('Confusion Matrixes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the confusion matrix if both events were completely independent?\n",
    "\n",
    "Remember that $P(0\\cap +)=\\frac{P(0\\cap +)}{N_T}=P(0|+)P(+)=P(0)P(+)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Pred -</th>\n",
    "    <th>Pred +</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Real 0</th>\n",
    "    <td>ùëÉ(0)ùëÉ(-)</td>\n",
    "    <td>ùëÉ(0)ùëÉ(+)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Real 1</th>\n",
    "    <td>ùëÉ(1)ùëÉ(-)</td>\n",
    "    <td>ùëÉ(1)ùëÉ(+)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_randomconfusion_matrix(y_pred,y_test):\n",
    "    ...\n",
    "    \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3,figsize=(15,5))\n",
    "for i,y_pred in enumerate([y_pred_r, y_pred_pca,y_pred_l]):\n",
    "    \n",
    "    cm=compute_randomconfusion_matrix(y_pred,y_test)\n",
    "\n",
    "    ax[i].set_title(titles[i])\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Blues\", cbar=False,ax=ax[i])\n",
    "    ax[i].set_xlabel('Predicted')\n",
    "    ax[i].set_ylabel('Real')\n",
    "plt.suptitle('Confusion Matrixes')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quality estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to compute the Sensitivity, Specificity, Precisiono, Negative Positive Value, Accuracy and F1 score, given two vectors of predicted and true labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_test, y_pred):\n",
    "    ...\n",
    "    \n",
    "    return Sen,Spe,Pre,NPV,Acc,F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the baseline for these estimators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_baseline(y_test, y_pred):\n",
    "    ...\n",
    "    \n",
    "    return Sen_ran,Spe_ran,Pre_ran,NPV_ran,Acc_ran,F1_ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quality_estimators(y_test, y_pred):\n",
    "    \n",
    "    Sen,Spe,Pre,NPV,Acc,F1= metrics(y_test, y_pred)\n",
    "    Sen_ran,Spe_ran,Pre_ran,NPV_ran,Acc_ran,F1_ran= metrics_baseline(y_test, y_pred)\n",
    "     \n",
    "    \n",
    "    print(\"**********************\")\n",
    "\n",
    "    print(f\"{'Accuracy:':<25} {Acc * 100:.2f}% ({Acc_ran * 100:.2f}%)\")\n",
    "    print(f\"{'F1 Score:':<25} {F1 * 100:.2f}% ({F1_ran * 100:.2f}%)\")\n",
    "    print(f\"{'Sensitivity (Recall):':<25} {Sen * 100:.2f}% ({Sen_ran * 100:.2f}%)\")\n",
    "    print(f\"{'Precision:':<25} {Pre * 100:.2f}% ({Pre_ran * 100:.2f}%)\")\n",
    "    print(f\"{'Specificity:':<25} {Spe * 100:.2f}% ({Spe_ran * 100:.2f}%)\")\n",
    "    print(f\"{'NPV:':<25} {NPV * 100:.2f}% ({NPV_ran * 100:.2f}%)\")\n",
    "\n",
    "    print(\"**********************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,y_pred in enumerate([y_pred_r, y_pred_pca,y_pred_l]):\n",
    "    print(titles[i])\n",
    "    quality_estimators(y_test, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality as a function of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the accuracy of each method as function of the parameters of the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=100 # number of parameters\n",
    "\n",
    "## random\n",
    "\n",
    "range_noskill=np.linspace(0, 1, N)\n",
    "Metrics_noskill=np.zeros((N,6))\n",
    "for ith,p in enumerate(range_noskill):\n",
    "    y_pred_th=no_skills(X_test,p=p)\n",
    "    Metrics_noskill[ith]=np.nan_to_num(metrics(y_test, y_pred_th))\n",
    "    \n",
    "plt.title(titles[0])\n",
    "plt.plot(range_noskill,Metrics_noskill[:,0],label='Sen')\n",
    "plt.plot(range_noskill,Metrics_noskill[:,1],label='Spe')\n",
    "plt.plot(range_noskill,Metrics_noskill[:,2],label='Pre')\n",
    "plt.plot(range_noskill,Metrics_noskill[:,3],label='NPV')\n",
    "plt.plot(range_noskill,Metrics_noskill[:,4],label='Acc')\n",
    "plt.plot(range_noskill,Metrics_noskill[:,5],label='F1')\n",
    "plt.xlabel('Probability +')\n",
    "plt.ylabel('metric')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## pca\n",
    "\n",
    "\n",
    "range_pca=np.linspace(np.min(X_train_pca[:, 0]), np.max(X_train_pca[:, 0]),N)\n",
    "Metrics_pca=np.zeros((N,6))\n",
    "for ith,p in enumerate(range_pca):\n",
    "    ...\n",
    "\n",
    "plt.title(titles[1])\n",
    "plt.plot(range_pca,Metrics_pca[:,0],label='Sen')\n",
    "plt.plot(range_pca,Metrics_pca[:,1],label='Spe')\n",
    "plt.plot(range_pca,Metrics_pca[:,2],label='Pre')\n",
    "plt.plot(range_pca,Metrics_pca[:,3],label='NPV')\n",
    "plt.plot(range_pca,Metrics_pca[:,4],label='Acc')\n",
    "plt.plot(range_pca,Metrics_pca[:,5],label='F1')\n",
    "plt.xlabel('Probability +')\n",
    "plt.ylabel('metric')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## logistic\n",
    "\n",
    "range_log=np.linspace(0, 1,N)\n",
    "Metrics_log=np.zeros((N,6))\n",
    "for ith,p in enumerate(range_log):\n",
    "    ...\n",
    "    \n",
    "\n",
    "plt.title(titles[2])\n",
    "plt.plot(range_log,Metrics_log[:,0],label='Sen')\n",
    "plt.plot(range_log,Metrics_log[:,1],label='Spe')\n",
    "plt.plot(range_log,Metrics_log[:,2],label='Pre')\n",
    "plt.plot(range_log,Metrics_log[:,3],label='NPV')\n",
    "plt.plot(range_log,Metrics_log[:,4],label='Acc')\n",
    "plt.plot(range_log,Metrics_log[:,5],label='F1')\n",
    "plt.xlabel('Probability +')\n",
    "plt.ylabel('metric')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "range_pca=np.linspace(np.min(X_train_pca[:, 0]), np.max(X_train_pca[:, 0]),N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the predictors, plot the Sensitivity vs. 1-Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(...,label='noskill')\n",
    "plt.plot(...,label=f'logistic')\n",
    "plt.plot(...,label=f'PCA')\n",
    "plt.plot([0,1],[0,1],':',color='black', label='uncorrelated')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can characterize the performance using the AUC (the area under the curve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "auc(1-Metrics_log[:,1], Metrics_log[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Best parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decide which is the best parameter by looking for the value for which we get quality values nearer to the perfect prediction, i.e. we can try to minimize \n",
    "\n",
    "$$dis=\\sqrt{(Sen-1)^2+(1-Spe)^2)}$$\n",
    "\n",
    "or maximize the distance to the uncorrelated threshold, the so-called Youden's index\n",
    "$$Youden=Sen-(1-Spe)=Sen+Spe-1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sen=Metrics_log[:,0]\n",
    "Spe=Metrics_log[:,1]\n",
    "\n",
    "dis=np.sqrt((Sen-1)**2+(1-Spe)**2)\n",
    "Youden=Sen+Spe-1\n",
    "\n",
    "... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the best parameters for the PCA and the Logistic regression and compare the quality metrics using these parameters with those discussed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
